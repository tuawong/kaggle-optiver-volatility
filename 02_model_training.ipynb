{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%load_ext autoreload"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%autoreload 2\r\n",
    "\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import joblib\r\n",
    "\r\n",
    "import os\r\n",
    "from tqdm import tqdm\r\n",
    "from glob import glob\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "\r\n",
    "import pathlib\r\n",
    "DATA_DIR = pathlib.Path.cwd()/'data/input'\r\n",
    "OUT_DIR = pathlib.Path.cwd()/'data/output'\r\n",
    "\r\n",
    "from sklearn.decomposition import PCA\r\n",
    "from sklearn.metrics import make_scorer, r2_score, mean_absolute_error\r\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, StratifiedKFold\r\n",
    "from sklearn.pipeline import Pipeline\r\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\r\n",
    "from sklearn.svm import SVR\r\n",
    "from sklearn.utils import resample\r\n",
    "\r\n",
    "import lightgbm as lgb\r\n",
    "from xgboost import XGBRegressor\r\n",
    "from skopt import BayesSearchCV\r\n",
    "from skopt.space import Real, Categorical, Integer\r\n",
    "from skopt.plots import plot_objective, plot_histogram, plot_convergence\r\n",
    "\r\n",
    "import sys \r\n",
    "sys.path.append(str(pathlib.Path.cwd()/'utils'))\r\n",
    "from utils.misc_utils import fullrange, realized_volatility, log_return, rmspe, get_stock_path, load_parquet_file, load_parquet_files, load_train_test\r\n",
    "\r\n",
    "rmspe_scorer = make_scorer(rmspe, greater_is_better=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "final_training_data = pd.read_pickle(OUT_DIR/'final_training_data_finer_buckets.pkl')\r\n",
    "final_test_data = pd.read_pickle(OUT_DIR/'final_test_data_finer_buckets.pkl')\r\n",
    "#final_training_data = final_training_data.dropna(axis=1)\r\n",
    "\r\n",
    "final_training_data['stock_id'] = final_training_data['stock_id'].astype(str)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "seed = 123\r\n",
    "model_col = [col for col in final_training_data.columns if ('id' not in col) & ('target' not in col)]\r\n",
    "\r\n",
    "X_train, X_test, y_train, y_test = train_test_split(\r\n",
    "                                        final_training_data.drop(['target', 'time_id', 'stock_id', 'id'], axis=1),\r\n",
    "                                        final_training_data['target'],\r\n",
    "                                        test_size=0.1, \r\n",
    "                                        random_state = seed\r\n",
    "                                        )\r\n",
    "\r\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\r\n",
    "                                        X_train,\r\n",
    "                                        y_train,\r\n",
    "                                        test_size=0.1, \r\n",
    "                                        random_state = seed\r\n",
    "                                        )\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_train.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "final_training_data.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Training "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "xgb = XGBRegressor()\r\n",
    "xgb.fit(X_train, y_train)\r\n",
    "rmspe(y_test, xgb.predict(X_test))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\r\n",
    "import lightgbm as lgb\r\n",
    "\r\n",
    "def rmspe_obj(\r\n",
    "    prediction,\r\n",
    "    train\r\n",
    "    ):\r\n",
    "    y = train.get_label()\r\n",
    "    grad = -2*(y-prediction)/(y**2)\r\n",
    "    hess = 2/(y**2)\r\n",
    "    return grad, hess\r\n",
    "\r\n",
    "def rmspe_eval(\r\n",
    "    prediction,\r\n",
    "    train\r\n",
    "    ):\r\n",
    "    y = train.get_label()\r\n",
    "    rmspe =  (np.sqrt(np.mean(np.square((y - prediction) / y))))\r\n",
    "    return 'rmspe', rmspe, False"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_data = lgb.Dataset(X_train, label=y_train)\r\n",
    "valid_data = lgb.Dataset(X_valid, label=y_valid)\r\n",
    "test_data = lgb.Dataset(X_test, label=y_test)\r\n",
    "\r\n",
    "\r\n",
    "parameters = {'verbosity': -1,\r\n",
    "                'n_jobs': -1,\r\n",
    "                'seed': 123}\r\n",
    "\r\n",
    "model = lgb.train(parameters,\r\n",
    "                       train_data,\r\n",
    "                       valid_sets=valid_data,\r\n",
    "                       fobj = rmspe_obj,\r\n",
    "                       feval = rmspe_eval,\r\n",
    "                       num_boost_round=50000,\r\n",
    "                       early_stopping_rounds=200)\r\n",
    "\r\n",
    "rmspe(y_test, model.predict(X_test))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Feature Importance"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import shap\r\n",
    "shap.initjs()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "explainer_lgbm = shap.KernelExplainer(model.predict, shap.sample(X_test, 100))\r\n",
    "shap_values_lgbm = explainer_lgbm.shap_values(shap.sample(X_test, 100), nsamples=500)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 8))\r\n",
    "shap.summary_plot(shap_values_lgbm, shap.sample(X_test, 100), plot_type=\"bar\", auto_size_plot=False, show=False)\r\n",
    "plt.tight_layout()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "vals= np.abs(shap_values_lgbm).mean(0)\r\n",
    "\r\n",
    "feature_importance = pd.DataFrame(list(zip(X_test.columns, vals)), columns=['col_name','feature_importance_vals'])\r\n",
    "feature_importance.sort_values(by=['feature_importance_vals'], ascending=False,inplace=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_train"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "num_features = 30\r\n",
    "selected_features = list(feature_importance.col_name)[:num_features]\r\n",
    "X_train_shap, X_valid_shap, X_test_shap = X_train[selected_features], X_valid[selected_features], X_test[selected_features]\r\n",
    "\r\n",
    "train_data_shap = lgb.Dataset(X_train_shap, label=y_train)\r\n",
    "valid_data_shap = lgb.Dataset(X_valid_shap, label=y_valid)\r\n",
    "test_data_shap = lgb.Dataset(X_test_shap, label=y_test)\r\n",
    "\r\n",
    "\r\n",
    "parameters = {'verbosity': -1,\r\n",
    "              'n_jobs': -1,\r\n",
    "              'seed': 123\r\n",
    "              }\r\n",
    "\r\n",
    "model = lgb.train(parameters,\r\n",
    "                       train_data_shap,\r\n",
    "                       valid_sets=valid_data_shap,\r\n",
    "                       fobj = rmspe_obj,\r\n",
    "                       feval = rmspe_eval,\r\n",
    "                       num_boost_round=50000,\r\n",
    "                       early_stopping_rounds=200,\r\n",
    "                       verbose_eval=0\r\n",
    "                       )\r\n",
    "\r\n",
    "rmspe(y_test, model.predict(X_test_shap))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### LOFO Importance"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import KFold\r\n",
    "from lofo import LOFOImportance, Dataset, plot_importance\r\n",
    "from sklearn.metrics import make_scorer\r\n",
    "\r\n",
    "rmspe_scorer = make_scorer(rmspe, greater_is_better=False)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# extract a sample of the data\r\n",
    "sample_df = X_test.copy() \r\n",
    "sample_df['target'] = y_test\r\n",
    "sample_df = sample_df.sample(frac=0.01, random_state=0)\r\n",
    "cv = KFold(n_splits=4, shuffle=True, random_state=0)\r\n",
    "# define the binary target and the features\r\n",
    "dataset = Dataset(df=sample_df, target=\"target\", features=[col for col in sample_df.columns if col != 'target'])\r\n",
    "# define the validation scheme and scorer. The default model is LightGBM\r\n",
    "lofo_imp = LOFOImportance(dataset, cv=cv, scoring=rmspe_scorer)\r\n",
    "# get the mean and standard deviation of the importances in pandas format\r\n",
    "importance_df = lofo_imp.get_importance()\r\n",
    "# plot the means and standard deviations of the importances\r\n",
    "plot_importance(importance_df[:50], figsize=(12, 20))\r\n",
    "plt.savefig('Importance Plot.png')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "selected_lofo_features = importance_df.loc[importance_df.importance_mean>0.001]['feature'].to_list()\r\n",
    "X_train_lofo, X_valid_lofo, X_test_lofo = X_train[selected_lofo_features], X_valid[selected_lofo_features], X_test[selected_lofo_features]\r\n",
    "\r\n",
    "train_data_lofo = lgb.Dataset(X_train_lofo, label=y_train)\r\n",
    "valid_data_lofo = lgb.Dataset(X_valid_lofo, label=y_valid)\r\n",
    "test_data_lofo = lgb.Dataset(X_test_lofo, label=y_test)\r\n",
    "\r\n",
    "parameters = {'verbosity': -1,\r\n",
    "                'n_jobs': -1,\r\n",
    "                'seed': 123}\r\n",
    "\r\n",
    "model = lgb.train(parameters,\r\n",
    "                       train_data_lofo,\r\n",
    "                       valid_sets=valid_data_lofo,\r\n",
    "                       fobj = rmspe_obj,\r\n",
    "                       feval = rmspe_eval,\r\n",
    "                       num_boost_round=50000,\r\n",
    "                       early_stopping_rounds=200,\r\n",
    "                       verbose_eval=0\r\n",
    "                       )\r\n",
    "\r\n",
    "rmspe(y_test, model.predict(X_test_lofo))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "submission = final_test_data[['id']].rename(columns = {'id': 'row_id'})\r\n",
    "submission['target'] = model.predict(final_test_data[selected_lofo_features])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation by Stocks"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.predict(X_test_lofo)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "stock_index_dict = final_training_data.reset_index().groupby('stock_id')['index'].apply(list).to_dict()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "score_dict_by_stock = {} \r\n",
    "for key in stock_index_dict.keys(): \r\n",
    "    mask = X_test_lofo.index.isin(stock_index_dict[key])\r\n",
    "    score_dict_by_stock[key] = rmspe(y_test[mask], model.predict(X_test_lofo[mask]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "score_dict_by_stock"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hyperparameter Tuning with Optuna"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import optuna\r\n",
    "from optuna.integration import LightGBMPruningCallback\r\n",
    "from sklearn.model_selection import KFold\r\n",
    "from utils.hyperparameter_tune import lightgbm_optuna_objective\r\n",
    "from utils.misc_utils import rmspe_eval, rmspe_obj, rmspe\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "seed = 123\r\n",
    "X = final_training_data.drop(['target', 'time_id', 'stock_id', 'id'], axis=1)\r\n",
    "y = final_training_data['target']\r\n",
    "\r\n",
    "cv = KFold(n_splits=5, random_state=seed, shuffle=True)\r\n",
    "\r\n",
    "fixed_params = parameters = {\r\n",
    "    'verbosity': -1,\r\n",
    "    'n_jobs': -1,\r\n",
    "    'seed': 123, \r\n",
    "    'metric': 'rmse'\r\n",
    "    }\r\n",
    "\r\n",
    "def objective(\r\n",
    "    trial, \r\n",
    "    X = final_training_data.drop(['target', 'time_id', 'stock_id', 'id'], axis=1), \r\n",
    "    y = final_training_data['target'], \r\n",
    "    fixed_params=fixed_params, \r\n",
    "    cv=cv\r\n",
    "    ):\r\n",
    "    \r\n",
    "    param_grid = {\r\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 15000, 25000, step=2000),\r\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\r\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 8, 4088, step=20),\r\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 14),\r\n",
    "        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 200, 500, step=100),\r\n",
    "        \"lambda_l1\": trial.suggest_int(\"lambda_l1\", 0, 10),\r\n",
    "        \"lambda_l2\": trial.suggest_int(\"lambda_l2\", 0, 10),\r\n",
    "        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.2, 0.95),\r\n",
    "        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.2, 0.95),\r\n",
    "        **fixed_params\r\n",
    "    }\r\n",
    "\r\n",
    "    pruning = LightGBMPruningCallback(trial, \"rmse\", valid_name='valid_1')\r\n",
    "    cv_score_rmspe = [] \r\n",
    "\r\n",
    "    for fold, (train_idx, test_idx) in enumerate(cv.split(X, y)):\r\n",
    "        X_train, X_valid = X.iloc[train_idx], X.iloc[test_idx]\r\n",
    "        y_train, y_valid = y[train_idx], y[test_idx]\r\n",
    "\r\n",
    "        train_data_cv = lgb.Dataset(X_train, label=y_train)\r\n",
    "        valid_data_cv = lgb.Dataset(X_valid, label=y_valid)\r\n",
    "        \r\n",
    "        model =  lgb.train(param_grid,\r\n",
    "            train_set=train_data_cv,\r\n",
    "            valid_sets=[train_data_cv, valid_data_cv],\r\n",
    "            early_stopping_rounds=100,\r\n",
    "            verbose_eval=0,   \r\n",
    "            fobj = rmspe_obj,\r\n",
    "            feval = rmspe_eval,\r\n",
    "            callbacks=[pruning]\r\n",
    "        )\r\n",
    "        predictions = model.predict(X_valid)\r\n",
    "        cv_score_rmspe.append(rmspe(predictions, y_valid))\r\n",
    "\r\n",
    "    return np.mean(cv_score_rmspe)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import functools\r\n",
    "\r\n",
    "optuna_obj = functools.partial(\r\n",
    "    lightgbm_optuna_objective,\r\n",
    "    X = final_training_data.drop(['target', 'time_id', 'stock_id', 'id'], axis=1), \r\n",
    "    y = final_training_data['target']\r\n",
    "    )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X = final_training_data.drop(['target', 'time_id', 'stock_id', 'id'], axis=1), \r\n",
    "y = final_training_data['target']\r\n",
    "\r\n",
    "study = optuna.create_study(direction='minimize', pruner=optuna.pruners.MedianPruner(n_warmup_steps=25))\r\n",
    "study.optimize(\r\n",
    "    objective,\r\n",
    "    timeout=180\r\n",
    "    )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X = final_training_data.drop(['target', 'time_id', 'stock_id', 'id'], axis=1), \r\n",
    "y = final_training_data['target']\r\n",
    "\r\n",
    "study = optuna.create_study(direction='minimize', pruner=optuna.pruners.MedianPruner(n_warmup_steps=25))\r\n",
    "study.optimize(\r\n",
    "    optuna_obj,\r\n",
    "    timeout=180\r\n",
    "    )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model_tuned =  lgb.train(params = new_params,\r\n",
    "    train_set= train_data,\r\n",
    "    valid_sets= [train_data, valid_data],\r\n",
    "    early_stopping_rounds=100,\r\n",
    "    verbose_eval=1,\r\n",
    "    fobj = rmspe_obj,\r\n",
    "    feval = rmspe_eval,\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.3",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "8306dfeffe99d5a396452c01da02b21b73a31270164607f570944acdd1bb5a92"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}